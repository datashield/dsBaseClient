#########################################################################################
# DataSHIELD Azure test suite.
# Starts with a vanilla Ubuntu VM, installs dsBase and dsBaseClient (as well as 
# dependencies - including a fully functional Opal server).
# Does checks and tests then saves results to testStatus repo.
#
# Inside the root directory $(Pipeline.Workspace) will be a file tree like:
# /datashield-infrastructure  <- Checked out version of datashield/datashield-infrastructure
# /dsBaseClient               <- Checked out version of datashield/dsBaseClient
# /testStatus                 <- Checked out version of datashield/testStatus
# /logs                       <- Where results of tests and lots are collated
#
# As of April 2020 this takes ~ 60 mins to run.
#
# The only things that should ever be changed are the repo branches in the resources.
#
#########################################################################################


#####################################################################################
# These should all be constant, except test_filter. This can be used to test subsets
# of test files in the testthat directory. Options are like:
# '*'               <- Run all tests
# 'ds.asNumeric*'   <- Run all ds.asNumeric tests, i.e. all the arg, smk etc tests.
# '*_smk_*'         <- Run all the smoke tests for all functions.
variables:
  datetime:    $[format('{0:yyyyMMddHHmmss}', pipeline.startTime)]
  repoName:    $(Build.Repository.Name)
  projectName: 'dsBaseClient'
  branchName:  $(Build.SourceBranchName)
  test_filter: '*'



#########################################################################################
# Need to define all the GH repos and their access tokens, see:
# https://docs.microsoft.com/en-us/azure/devops/pipelines/library/service-endpoints?view=azure-devops&tabs=yaml
resources:
  repositories:
  - repository: testStatusRepo
    type: github
    endpoint: datashield-testing
    name: datashield/testStatus
    ref: master
    
  - repository: datashield-infrastructureRepo
    type: github
    endpoint: datashield-testing
    name: datashield/datashield-infrastructure
    ref: v6.0-dev



#########################################################################################
# When and under what condition to run the pipeline.
schedules:
  - cron: "32 1 * * *"
    displayName: Nightly build
    branches:
     include:
      - master
    always: true

jobs:
- job: build_and_run_tests
  timeoutInMinutes: 120
  pool:
    vmImage: 'Ubuntu 16.04'


  steps:
    #####################################################################################
    # Checkout the source code to a subfolder.
    # This may give an error in the logs like:
    # [warning]Unable move and reuse existing repository to required location
    # This is an Azure bug - https://github.com/microsoft/azure-pipelines-yaml/issues/403
  - checkout: self
    path: 'dsBaseClient'

  - checkout: datashield-infrastructureRepo
    path: 'datashield-infrastructure'

  - checkout: testStatusRepo
    path: 'testStatus'
    persistCredentials: true
    condition: and(eq(variables['Build.Repository.Name'], 'datashield/dsBaseClient'), ne(variables['Build.Reason'], 'PullRequest'))



    #####################################################################################
    # The MySQL install that comes with the VM doesn't seem compatable with our set up
    # so we delete it.
    # If previous steps have failed then don't run.
  - bash: |
  
      # Purge the default mysql installed on the VM as it is incompatible with our stuff.
      sudo service mysql stop
      sudo apt-get update
      sudo apt-get remove --purge mysql-client mysql-server mysql-common -y
      sudo apt-get purge mysql-client mysql-server mysql-common -y
      sudo apt-get autoremove -y
      sudo apt-get autoclean -y
      sudo rm -rf /var/lib/mysql/
  
    displayName: 'Uninstall default MySQL'
    condition: succeeded()


    #####################################################################################
    # The Azure VMs have 2 CPUs, so configure R to use both when compile/install packages.
    # If previous steps have failed then don't run.
  - bash: |

      echo "options(Ncpus=2)" >> ~/.Rprofile

    displayName: 'Tweak local R env using .Rprofile'
    condition: succeeded()


    #####################################################################################
    # Install R and all the dependencies dsBaseClient requires.
    # If previous steps have failed then don't run.
  - bash: |
      
      sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
      sudo add-apt-repository "deb https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/"
      sudo apt-get update
      
      sudo apt-get install -qq libxml2-dev libcurl4-openssl-dev libssl-dev libgsl-dev r-base -y
      sudo R -q -e "install.packages(c('devtools','metafor','fields','covr'), dependencies=TRUE, repos='https://cloud.r-project.org')"              
      sudo R -q -e "install.packages(c('DSI','DSOpal','DSLite'), dependencies=TRUE, repos='https://cloud.r-project.org')"

    displayName: 'Install all dependencies for dsBaseClient'
    condition: succeeded()


    #####################################################################################
    # Check that the man files in the repo match what is in the function headers. i.e. has
    # devtools::document() been run before commiting?
    # If previous steps have failed then don't run.
    # If this step fails still mark as failed, but don't stop the rest of the steps running.
  - bash: |

      # Concatenate all the files in the man dir into one long string and md5sum it.
      orig_sum=$(find man -type f | sort -u | xargs cat | md5sum)

      # Rebuild the documentation.
      R -e "devtools::document()"

      # Concatenate all the files in the man dir into one long string and md5sum it.
      new_sum=$(find man -type f | sort -u | xargs cat | md5sum)

      if [ "$orig_sum" != "$new_sum" ]; then
        echo "Your committed manual files (man/*.Rd) are out of sync with the documentation in the R files."
        echo "Run devtools::document() locally then commit again."
        exit 1
      else
       echo "Documentation up to date."
       exit 0
      fi
      
    workingDirectory: $(Pipeline.Workspace)/dsBaseClient
    displayName: 'Check manual updated before being committed'
    condition: succeeded()
    continueOnError: true


    #####################################################################################
    # Run devtools::check on the checked out source code. 
    # If previous steps have failed then don't run.
    # If this step fails still mark as failed, but don't stop the rest of the steps running.    
  - bash: |
  
      R -q -e "library('devtools'); devtools::check(args = c('--no-examples'))" | tee azure-pipelines_check.Rout
      grep --quiet "^0 errors" azure-pipelines_check.Rout && grep --quiet " 0 warnings" azure-pipelines_check.Rout && grep --quiet " 0 notes" azure-pipelines_check.Rout

    workingDirectory: $(Pipeline.Workspace)/dsBaseClient
    displayName: 'Devtools checks'
    condition: succeeded()
    continueOnError: true


    #####################################################################################
    # Install dsBase. This uses the datashield-infrastructure repo (and its dependencies) to do the install with puppet.
    # If previous steps have failed then don't run.
  - bash: |
  
      # Set up puppet and r10k
      wget -nv https://apt.puppetlabs.com/puppet5-release-xenial.deb
      sudo dpkg -i puppet5-release-xenial.deb
      sudo apt-get install -qq -f
      sudo apt-get update
      sudo rm -f puppet5-release-xenial.deb

      sudo apt-get install puppet-agent -y
      sudo /opt/puppetlabs/puppet/bin/gem install r10k
      
      pushd puppet/environments/datashield_azurepipelines
      sudo /opt/puppetlabs/puppet/bin/r10k puppetfile install
      popd
      
      sudo /opt/puppetlabs/bin/puppet apply $(Pipeline.Workspace)/dsBaseClient/azure-pipelines_site.pp --environment datashield_azurepipelines --environmentpath puppet/environments

    workingDirectory: $(Pipeline.Workspace)/datashield-infrastructure
    displayName: 'Install DataSHIELD server (opal, dsBase, etc)'
    condition: succeeded()


    #####################################################################################
    # Essentially run devtools::test() on the checked out code. This is wrapped up with
    # code coverage. The actual command is vary convoluted as it had to do some things
    # which are not default behaviour: output the results to a JUnit xml file, not stop
    # when a small number of errors have happened, run through the code coverage tool.
    # TODO: Tidy up variable names - use timestamps here.
    # TODO: Why is DSLite needed for this to run?!
  - bash: |
  
      # There is an issue with the way we are using packages. The wrapped up test command
      # below fails in a way that implies that it is not installed. I cannot figure out 
      # why this is case. As a work around we can run some of the functions below. My
      # best guess is that there is an implicit build or similar that happens. Although
      # I cannot replicate that directly with build etc directly.
      
      # None of the below make the tests run
      #sudo R -e 'devtools::reload(".")'
      #sudo R -e 'library(dsBaseClient)'
      #sudo R -e 'devtools::check_built()'
      #sudo R --verbose -e 'devtools::build()'      
      
      # Any of the below makes the tests work.
      #sudo R --verbose -e 'devtools::test()'
      #sudo R --verbose -e 'devtools::install()'
      sudo R --verbose -e 'devtools::check()'            
      
      pwd
      mkdir $(Pipeline.Workspace)/logs

      # run the coverage tool and output to coveragelist.csv
      # testthat::testpackage uses a MultiReporter, comprised of a ProgressReporter and JunitReporter
      # R output and messages are redirected by sink() to test_console_output.txt
      # junit reporter output is to test_results.xml
      sudo R -q -e '
        library(covr);
        write.csv(
                  coverage_to_list(
                                   covr::package_coverage(
                                                          type = c("none"),
                                                          code = c(
                                                                   '"'"'library(testthat); 
                                                                   output_file <- file("test_console_output.txt"); 
                                                                   sink(output_file); 
                                                                   sink(output_file, type = "message"); 
                                                                   library(testthat); 
                                                                   junit_rep <- JunitReporter$new(file = "test_results.xml"); 
                                                                   progress_rep <- ProgressReporter$new(max_failures = 999999); 
                                                                   multi_rep <- MultiReporter$new(reporters = list(progress_rep, junit_rep)); 
                                                                   testthat::test_package("$(projectName)", filter = "$(test_filter)",reporter = multi_rep, stop_on_failure = FALSE)'"'"')
                                                         )
                                  ),
                                  "coveragelist.csv"
                 )'
      
      # display the test console output
      cat test_console_output.txt
      mv coveragelist.csv $(Pipeline.Workspace)/logs
      mv test_results.xml $(Pipeline.Workspace)/logs
      mv test_console_output.txt $(Pipeline.Workspace)/logs
      
      grep --quiet "Failed:   0" $(Pipeline.Workspace)/logs/test_console_output.txt

    workingDirectory: $(Pipeline.Workspace)/dsBaseClient
    displayName: 'Code coverage and JUnit report output'
    condition: succeeded()


    #####################################################################################
    # Parse the JUnit file to see if there are any errors/warnings. If there are then 
    # echo them so finding bugs should be easier.
    # This should run even if previous steps have failed.
  - bash: |
  
      # Strip out when error and failure = 0 and count the number of times it does not.
      issue_count=$(sed 's/failures="0" errors="0"//' test_results.xml | grep --count errors=)
      echo "Number of testsuites with issues: "$issue_count
      echo "Testsuites with issues:"
      sed 's/failures="0" errors="0"//' test_results.xml | grep errors= > issues.log
      cat issues.log
      exit $issue_count

    workingDirectory: $(Pipeline.Workspace)/logs
    displayName: 'Check for errors & Failures in JUnit file'
    condition: succeededOrFailed()


    #####################################################################################
    # Output some important version numbers to file. This gets added to the testStatus 
    # commit so it can be parsed and used on the status table.
  - bash: |
  
      echo 'branch:'$(branchName) >> $(datetime).txt
      echo 'os:'$(lsb_release -ds) >> $(datetime).txt
      echo 'R:'$(R --version | head -n 1) >> $(datetime).txt
      echo 'opal:'$(opal system --opal localhost:8080 --user administrator --password "datashield_test&" --version) >> $(datetime).txt

    workingDirectory: $(Pipeline.Workspace)/logs
    displayName: 'Write versions to file'
    condition: succeededOrFailed()


    #####################################################################################
    # Checkout the testStatus repo, add the results from here, push back to GH.
    # TODO: Automatically pull in better email/name info from somewhere.
    # TODO: More debug info in commit message
  - bash: |
  
      # Git needs some config set to be able to push to a repo. 
      git config --global user.email "you@example.com"
      git config --global user.name "Azure pipeline"

      # This repo is checked out in detatched head state, so reconnect it here.
      git checkout master
      
      # It is possible that other commits have been made to the testStatus repo since it 
      # was checked out. i.e. other pipeline runs might have finished.
      git pull

      # Make the directories if they dont already exist
      mkdir --parents logs/$(projectName)/$(branchName)
      mkdir --parents docs/$(projectName)/$(branchName)/latest

      cp $(Pipeline.Workspace)/logs/coveragelist.csv logs/$(projectName)/$(branchName)/
      cp $(Pipeline.Workspace)/logs/coveragelist.csv logs/$(projectName)/$(branchName)/$(datetime).csv

      cp $(Pipeline.Workspace)/logs/test_results.xml logs/$(projectName)/$(branchName)/
      cp $(Pipeline.Workspace)/logs/test_results.xml logs/$(projectName)/$(branchName)/$(datetime).xml

      # Run the script to parse the results and build the html pages.
      # status.py JUnit_file.xml coverage_file.csv output_file.html local_repo_path remote_repo_name branch
      source/status.py logs/$(projectName)/$(branchName)/$(datetime).xml logs/$(projectName)/$(branchName)/$(datetime).csv status.html $(Pipeline.Workspace)/$(projectName) $(projectName) $(branchName)

      cp status.html docs/$(projectName)/$(branchName)/latest/index.html

      git add logs/$(projectName)/$(branchName)/coveragelist.csv
      git add logs/$(projectName)/$(branchName)/test_results.xml
      git add logs/$(projectName)/$(branchName)/$(datetime).xml
      git add logs/$(projectName)/$(branchName)/$(datetime).csv
      git add docs/$(projectName)/$(branchName)/latest/index.html

      # copy versions file into repo tree and add
      cp $(Pipeline.Workspace)/logs/$(datetime).txt logs/$(projectName)/$(branchName)/
      git add logs/$(projectName)/$(branchName)/$(datetime).txt

      git commit -m "Azure auto test for $(projectName)/$(branchName) @ $(datetime)" -m "Debug info:\nProjectName:$(projectName)\nBranchName:$(branchName)\nDataTime:$(datetime)"
      git push
      exit 0
      
    workingDirectory: $(Pipeline.Workspace)/testStatus
    displayName: 'Parse test results'
    condition: and(eq(variables['Build.Repository.Name'], 'datashield/dsBaseClient'), ne(variables['Build.Reason'], 'PullRequest'))


    #####################################################################################
    # Output the environment information to the console. This is useful for debugging.
    # Always do this, even if some of the above has failed or the job has been cacelled.
  - bash: |
  
      echo 'BranchName: '$(branchName)
      echo 'ProjectName: '$(projectName)
      echo 'RepoName: '$(repoName)
      
      echo -e "\n#############################"
      echo -e "ls /: ######################"
      ls $(Pipeline.Workspace)
    
      echo -e "\n#############################"
      echo -e "lscpu: ######################"
      lscpu
      
      echo -e "\n#############################"
      echo -e "memory: #####################"
      free -m
      
      echo -e "\n#############################"
      echo -e "env: ########################"
      env
      
      echo -e "\n#############################"
      echo -e "Puppet version: #############"
      /opt/puppetlabs/bin/puppet --version
      /opt/puppetlabs/puppet/bin/r10k version
      
      echo -e "\n#############################"
      echo -e "Rprofile: ###################"
      cat $(Pipeline.Workspace)/dsBaseClient/.Rprofile
      
      echo -e "\n#############################"
      echo -e "R installed.packages(): #####"
      R -e 'installed.packages()'
      
      echo -e "\n#############################"
      echo -e "R sessionInfo(): ############"
      R -e 'sessionInfo()'

      sudo apt install tree -y
      pwd
      echo -e "\n#############################"
      echo -e "File tree: ##################"
      tree $(Pipeline.Workspace)

    displayName: 'Environment info'
    condition: always()
